#Created on July 23, 2015
#@author: rspies
# Lynker Technologies
# Python 2.7
# Description: parse through a csv file with date and variable and generate a 
# formatted datacard for chps import. Note: script does not check for missing 
# time steps

import os
import datetime
import collections
maindir = os.getcwd()
os.chdir("../..")
maindir = os.getcwd() + os.sep

################### user input #########################
RFC = 'WGRFC_FY2017'
station_dir = maindir + 'Calibration_NWS\\' + RFC[:5] + os.sep + RFC + '\\data_csv\\IBWC'
csv_dir = station_dir + os.sep + 'qme_csv' + os.sep 
out_dir = station_dir + os.sep + 'qme_datacard'
variable = 'QME'
csv_units = 'CMS' # define units in csv file in case conversion is needed
chps_shift = 'no' # shift time back by 1 timestep (csv from CHPS shifts forward compared to original datacard)
variable_name = 'QME' #'inflow' or 'outflow'
date_col = 0 # csv column with date
data_col = 1 # csv column with data
start_limit = datetime.datetime(1924,1,1,0)
header_lines = 1 # header lines in csv
timestep = 24 # time step in hours
date_time_format = "%m/%d/%Y %H:%M"
#data_format = 'f9.3'

loc_name = {'CMRT4':'Rio Alamo at Cuidad Mier',
'RRGT2':'Rio Grande near Roma',
'RGDT2':'Rio Grande at Fort Ringgold',
'RGLT2':'Rio Grande near Los Ebanos',
'MITT2':'Banker Floodway near Mission',
'PRGT2':'Rio Grande near Progresso',
'SBMT2':'Rio Grande near San Benito',
'LOBT2':'Rio Grande near Brownsville',
'WSLT2':'Rio Grande Main US Floodway near Westlaco',
'MDET2':'North Rio Grande Floodway at Mercedes (if available)',
'SBST2':'North Rio Grande Floodway near Sebastian',
'MECT2':'Arroyo Colorado Floodway near Mercedes',
'HAGT2':'Arroyo Colorado Floodway near Harlingen',
'CMGTP':'RIO SAN JUAN AT CAMARGO TAMAULIPAS',
'MADT2A':'ANZALDUAS CANAL NEAR REYNOSA',
'MADT2':'RIO GRANDE BELOW ANZALDUAS DAM NEAR REYNOSA'} #
########################################################
var_units = {'QME':'CFSD','MAP':'MM','MAT':'DEGF'}
var_dim = {'QME':'L3','MAP':'L','MAT':'TEMP'}

basin_list = [] # name of location
for each in os.listdir(csv_dir):
    basin_list.append(each.rstrip('.csv'))
print basin_list

for basin_id in basin_list:
    count = 0; date_count = 0
    csv_file = csv_dir + basin_id + '.csv'
    read_file = open(csv_file,'r')
    dates = collections.OrderedDict()
    date_check = []
    print 'parsing file: ' + basin_id + ' -> ' + csv_file
    for line in read_file:
        count += 1
        if count > header_lines:
            date_count += 1
            sep = line.split(',')
            date = datetime.datetime.strptime(sep[date_col], date_time_format)
            if timestep == 24:
                date = date.replace(hour=12, minute=0) # set uniform hour/minute for daily data processing
            if chps_shift == 'yes':
                date = date - datetime.timedelta(hours=timestep)
            data = sep[data_col].strip()
            if data == '' or data == 'M' or data == 'M\n':
                data = -999
            elif float(data) < 0 and variable != 'MAT':
                data = -999
            if date >= start_limit:
                if data != -999 and variable == 'QME' and csv_units == 'CMS':
                    data = str(float(data)*35.3147) # convert QME from cms to cfs
                dates[date] = data
                if date_count == 1:
                    start_date = date.replace(day=1) # get first date of data and set day to 1 (ensure full month in datacard)
                end_date = date # get last date of data
    while start_date <= end_date: # create list of all possible data points to ensure no missing time steps
        if start_date not in dates:
            data = -999
            dates[start_date] = data
        date_check.append(start_date)
        start_date = start_date + datetime.timedelta(hours=timestep)
    #### data output to formatted datacard
    write_file = open(out_dir + os.sep +  basin_id + '_' + variable.upper() + '_converted' + '.txt','wb')
    write_file.write('$ Datacard Time Series created at ' + str(datetime.datetime.now().date()) + ' from CSV Conversion Script\n')
    write_file.write('$ Generated by Lynker Technologies\n' + '$ RFC Basin Calibration Task\n')
    write_file.write('$ Data type: ' + variable_name + '\n')
    write_file.write('$ PERIOD OF RECORD= ' + str(min(dates).month)+'/'+str(min(dates).year) + ' THRU ' + str(max(dates).month)+'/'+str(max(dates).year)+'\n')
    #write_file.write('$ \n')
    write_file.write('$ Symbol for missing data = -999.00 \n')
    write_file.write('$ ' + 'TYPE=' + variable + '    ' + 'UNITS=' + var_units[variable] + '    ' + 'DIMENSIONS=' + variable_name + '    ' + 'TIME INTERVAL=' + str(timestep) + ' HOURS\n')
    write_file.write('{:12s}  {:4s} {:4s} {:4s} {:2d}   {:12s}    {:12s}'.format('DATACARD', variable, var_dim[variable],var_units[variable],int(timestep),basin_id,loc_name[basin_id]))
    write_file.write('\n')
    #write_file.write('datacard      ' + variable + '  ' + var_dim[variable] + '   '+ var_units[variable] +'   ' + str(timestep) + '   '+basin_id+'          '+loc_name+'\n')
    if min(dates).month >=10 and max(dates).month >= 10:
        write_file.write(str(min(dates).month)+'  '+str(min(dates).year)+' '+str(max(dates).month)+'   '+str(max(dates).year)+'  1   F9.3'+'\n')    
    elif min(dates).month >=10:
        write_file.write(str(min(dates).month)+'  '+str(min(dates).year)+'  '+str(max(dates).month)+'   '+str(max(dates).year)+'  1   F9.3'+'\n')
    elif max(dates).month >=10:
        write_file.write(' ' + str(min(dates).month)+'  '+str(min(dates).year)+' '+str(max(dates).month)+'   '+str(max(dates).year)+'  1   F9.3'+'\n')
    else:
        write_file.write(' ' + str(min(dates).month)+'  '+str(min(dates).year)+'  '+str(max(dates).month)+'   '+str(max(dates).year)+'  1   F9.3'+'\n')
    month_prev = min(dates).month
    hr_count = 0
    for each in date_check: # loop through complete date list to ensure chronological order
        if each.month == month_prev:
            hr_count += 1
        else:
            hr_count = 1
        if int(each.month) < 10:
            space1 = '    '
        else:
            space1 = '   '
        if hr_count < 10:
            space2= '   '
        elif hr_count <100:
            space2= '  '
        else:
            space2= ' '
        ### write data to datacard ###
        if float(dates[each]) <= 9999.99:
            write_file.write('{:12s}{:2d}{:02d}{:4d}{:9.3f}'.format(basin_id,int(each.month),int(str(each.year)[-2:]),hr_count,float(dates[each])))
        elif float(dates[each]) <= 99999.99:
            write_file.write('{:12s}{:2d}{:02d}{:4d}{:10.3f}'.format(basin_id,int(each.month),int(str(each.year)[-2:]),hr_count,float(dates[each])))
        elif float(dates[each]) <= 999999.99:
            write_file.write('{:12s}{:2d}{:02d}{:4d}{:11.3f}'.format(basin_id,int(each.month),int(str(each.year)[-2:]),hr_count,float(dates[each])))
            print '!!!Significant value found: ' + str(each.month) + '/' + str(each.year) + ' ' + str(hr_count) + str(dates[each]) +'\n'
        else:
            print '!!!Significant too long for format: ' + str(each.month) + '/' + str(each.year) + ' ' + str(hr_count) + str(dates[each]) +'\n'
        write_file.write('\n')
        #write_file.write(basin_id + space1 + str(each.month)+str(each.year)[2:] + space2 +  str(hr_count) + "%10.2f" % float(dates[each]) + '\n')
        month_prev = each.month
    write_file.close()
    read_file.close()
print 'Completed!'